#!/bin/bash

# 30.03.2019 16:58:07 EDT

########################################
# DEFINE GLOBAL ENVIRONMENT PARAMETERS #
########################################

# For all snakemake based pipelines we will at least need to know the location of the
# repository itself, as well as a per-user central conda environment work directory.
# If we do not specify the latter, the conda environments cannot easily be reused
# between jobs and a new environment will be created in each folder that the pipeline
# runs in. These would then need to be cleared manually each time.

# Set path to parent dir for conda environment work directory and git repositories
CONDA_WORKDIR="${CONDA_WORKDIR:=/sc/arion/work/$USER/snakemake}"
GIT_REPODIR="${GIT_REPODIR:=$HOME/opt}"
JOB_NCPUS="${JOB_NCPUS:=24}"

# Make sure the ngs-tools repository exists at the expected location
if [[ ! -d "${GIT_REPODIR}/ngs-tools" ]]
then
   echo "Error: could not find the ngs-tools folder in the defined repository location:\n  ${GIT_REPODIR}"
   exit 2
fi

# Make sure the igb-tools repository exists at the expected location
if [[ ! -d "${GIT_REPODIR}/igb-tools" ]]
then
   echo "Error: could not find the igb-tools folder in the defined repository location:\n  ${GIT_REPODIR}"
   exit 2
fi

# Make sure the utility repository exists at the expected location
if [[ ! -d "${GIT_REPODIR}/utility" ]]
then
   echo "Error: could not find the utility folder in the defined repository location:\n  ${GIT_REPODIR}"
   exit 2
fi

# Make sure the minerva-queue-lsf repository exists at the expected location
if [[ ! -d "${GIT_REPODIR}/minerva-queue-lsf" ]]
then
   echo "Error: could not find the minerva-queue-lsf folder in the defined repository location:\n  ${GIT_REPODIR}"
   exit 2
fi

# Create the conda environment work directory if it doesn't already exist
if [[ ! -d "${CONDA_WORKDIR}" ]]
then
   mkdir -p "${CONDA_WORKDIR}"
fi

# Make sure we have a conda environment work directory at this point (the directory creation could have failed)
if [[ ! -d "${CONDA_WORKDIR}" ]]
then
   echo "Error: could not find the conda environment work directory at the defined location:\n  ${CONDA_WORKDIR}"
   exit 2
fi

# Export environment parameters
export CONDA_WORKDIR
export GIT_REPODIR


############################
# PROCESS COMMANDLINE ARGS #
############################

# Process command line arguments
PREFIX=""
DEBUG=""
CONFIGFILE=""
TOUCH=""
TASK="all"

while getopts "i:s:DC:T" opt; do
   case $opt in
       
   i)
      PREFIX="$OPTARG"
      ;;

   s) TASK="$OPTARG"
      ;;

   D)
      DEBUG="-p -n"
      ;;

   C)
      CONFIGFILE="--configfile $OPTARG"
      ;;

   T) 
      TOUCH="--touch"
      ;;

   *)
      echo "Incorrect options provided"
      exit 1
      ;;

   esac
done

# Check arguments and display a help message if the required arguments are not found
if [ -z "$PREFIX" ];
then
  cat << EOF

   Usage: run-post-niap-merge -s <TARGET> -i <PREFIX>

   Arguments:
    -i <string>
      File prefix for the .gtf and .counts file
    -D 
      Run pipeline in debug mode to show the commands that will be executed.
    -T
      Touch all analysis outputs without rerunning the pipeline. Use this option
      when changes are made to the pipeline that do not impact outputs. This
      avoids rerunning analysis steps unnecessarily.
    -C <configfile>
      Path to configfile to override the default yaml config
    -s <string>
      Analysis target. By default the pipeline will run the 'all' target
      to run the basic analyses that are less time-consuming. The 'extra'
      target will additionally run transdecoder, pfam, interproscan, and cpatv3
      on isoforms to identify ORFs and functional domains.
    
    -help
      This help message

EOF
  exit 0
fi


############################
# PREPARE THE JOB LOG FILE #
############################

# Here we write to a general log file for the pipeline that is timestamped
# to the start of the run. When run on the cluster, the LSF job ID is also
# added to the pipeline log file name.

# Write basic analysis parameters to a log file
timestamp="$(date +"%F_%H-%M-%S")"
if [ -n "$LSB_JOBID" ]
then
   timestamp="${timestamp}_lsf${LSB_JOBID}"
fi
echo "## Starting post-niap analysis with the following parameters:" > pipeline_${timestamp}.log
echo "   File prefix:      $PREFIX"         >> pipeline_${timestamp}.log
echo "   Config file:      $CONFIGFILE"     >> pipeline_${timestamp}.log

# Generate a random name for the jobs
JOBNAME=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 5 | head -n 1)


#################################
# PREPARE SNAKEMAKE ENVIRONMENT #
#################################

# Load snakemake conda environment on chimera
module purge all
unset PYTHONPATH
unset PERL5LIB
unset R_LIBS
module load anaconda3
source activate snakemake

# Set http(s) proxy server env variables
module load proxies

# Add the bin folder with any external scripts that may be called from the pipeline to the path
export PATH="$GIT_REPODIR/sm_post-niap-merge/bin/:$PATH"


############################
# START SNAKEMAKE PIPELINE #
############################

# Prepare the snakemake command
cmd="snakemake \
${DEBUG} \
${CONFIGFILE} \
${TOUCH} \
--use-conda \
--conda-frontend mamba \
--conda-prefix   ${CONDA_WORKDIR} \
--snakefile      ${GIT_REPODIR}/sm_post-niap-merge/workflow/Snakefile \
--cores          ${JOB_NCPUS} \
--config         prefix=${PREFIX} --
${TASK}"

# Make sure the exact command that was run ends up in the pipeline log
echo -e "\n## Run command\n$cmd\n\n## Pipeline output" >> pipeline_${timestamp}.log

# Run pipeline
$cmd >>  pipeline_${timestamp}.log 2>&1

# Check if the pipeline completed without errors
if [ $? -eq 0 ]
then
   sleep 5
   echo -e "\n#########################################\n# POST NIAP PIPELINE ENDED SUCCESSFULLY #\n#########################################\n" >> pipeline_${timestamp}.log;
else
   sleep 5
   echo -e "\n########################################\n# POST NIAP PIPELINE ENDED WITH ERRORS #\n########################################\n" >> pipeline_${timestamp}.log;
fi

